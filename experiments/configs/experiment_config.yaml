# Experiment Configuration

models:
  - "meta-llama/Llama-2-7b-chat-hf"
  - "mistralai/Mistral-7B-Instruct-v0.2"

evaluation:
  num_variance_runs: 5
  temperature: 0.7
  max_tokens: 256

test_cases:
  contradictory_contexts:
    enabled: true
    num_samples: 3
  
  response_variance:
    enabled: true
    num_samples: 3

output:
  save_individual_responses: true
  save_aggregated_metrics: true
  results_dir: "results/metrics"